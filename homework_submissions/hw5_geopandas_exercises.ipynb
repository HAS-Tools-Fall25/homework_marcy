{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the geopandas homework! \n",
    "In this homework you will learn more about how to use geopandas to work with geospatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Some setup\n",
    "NOTE: Here we are just  pulling in data from GitHub directly.\n",
    "This can be done by specifying the url to the shapefile, but\n",
    "prepending it with `/vsicurl`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "../data/arizona_shapefile/tl_2016_04_cousub.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDataSourceError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m az = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/arizona_shapefile/tl_2016_04_cousub.shp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m gages = gpd.read_file(\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m../data/gagesii_shapefile/gagesII_9322_sept30_2011.shp\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m huc8 = gpd.read_file(\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m../data/arizona_huc8_shapefile/WBDHU8.shp\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/homework_marcy/.venv/lib/python3.11/site-packages/geopandas/io/file.py:316\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m             filename = response.read()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/homework_marcy/.venv/lib/python3.11/site-packages/geopandas/io/file.py:576\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     warnings.warn(\n\u001b[32m    568\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    572\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/homework_marcy/.venv/lib/python3.11/site-packages/pyogrio/geopandas.py:275\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[32m    273\u001b[39m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[32m    274\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdatetime_as_string\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m result = \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/homework_marcy/.venv/lib/python3.11/site-packages/pyogrio/raw.py:198\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    196\u001b[39m dataset_kwargs = _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:1313\u001b[39m, in \u001b[36mpyogrio._io.ogr_read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpyogrio/_io.pyx:232\u001b[39m, in \u001b[36mpyogrio._io.ogr_open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDataSourceError\u001b[39m: ../data/arizona_shapefile/tl_2016_04_cousub.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "az = gpd.read_file(\n",
    "    '../data/arizona_shapefile/tl_2016_04_cousub.shp')\n",
    "gages = gpd.read_file(\n",
    "    '../data/gagesii_shapefile/gagesII_9322_sept30_2011.shp')\n",
    "huc8 = gpd.read_file(\n",
    "    '../data/arizona_huc8_shapefile/WBDHU8.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: \n",
    "Put the `gages` geodataframe onto the same CRS as the `az` geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: \n",
    "The various polygons in the Arizona shapefile\n",
    "are just census designated boundaries, and don't really\n",
    "mean anything as far as the hydrology of Arizona. So,\n",
    "let's get rid of them. In GIS-language this is called\n",
    "\"dissolving\" the polygons. \n",
    "\n",
    "Your task here is to make the `az` variable be a \n",
    "geodataframe with only a single geometry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: \n",
    "Pull out only the gages in Arizona from \n",
    "the `gages` dataset, save this as `az_gages`\n",
    "In GIS-language this is called \"clipping\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here\n",
    "az_gages = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: \n",
    "Make a plot showing Arizona in \"lightgrey\"\n",
    "and the locations of the gages in Arizona plotted as\n",
    "\"crimson\" colored points.\n",
    "\n",
    "NOTE: Calling `.plot` on a geodataframe will return \n",
    "      a new axis object which can be passed to \n",
    "      subsequent plot commands \n",
    "\n",
    "NOTE: You might try setting `markersize=3` or similar\n",
    "      when you are plotting the gages, so that it's \n",
    "      easier to see them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: \n",
    "I also gave you a dataset of watershed\n",
    "boundaries (called HUCs, for hydrologic unit code).\n",
    "I gave you the \"level 8\" units, where a smaller unit\n",
    "level means a larger spatial aggregation, and a larger\n",
    "code is more fine-scaled. This is stored in the variable \n",
    "`huc8`. \n",
    "\n",
    "Plot the huc8 boundaries in \"lightgrey\", then plot \n",
    "the outline of Arizona over the top of it. Finally\n",
    "plot the gages contained in Arizona again as \"crimson\"\n",
    "points.\n",
    "\n",
    "NOTE: To get a transparent \"face color\" for the Arizona\n",
    "      outline set `color=\"none\"` and `edgecolor=\"black\"`\n",
    "      inside of your second plot command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:  \n",
    "For this step, I want you to plot the location\n",
    "of the Verde river gage that we've been using as an example. \n",
    "\n",
    "To do this, first find the row where # the `'STANAME'` \n",
    "column of `az_gages` is equal to # the `name` variable. \n",
    "Then use that information to select out only the Verde\n",
    "river gage into the variable `verde_gage`.\n",
    "\n",
    "The resulting plot should put a big star where the \n",
    "gage location is. All other gages in Arizona will\n",
    "still appear as dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"VERDE RIVER NEAR CAMP VERDE, AZ\"\n",
    "# TODO: Your code here\n",
    "is_the_gage = None\n",
    "verde_gage = None\n",
    "\n",
    "# Plotting code, you should not have to modify\n",
    "ax = huc8.plot(color='lightgrey')\n",
    "az.plot(ax=ax, edgecolor='black', color=\"none\")\n",
    "az_gages.plot(ax=ax, color='rosybrown', markersize=3)\n",
    "verde_gage.plot(ax=ax, color='crimson', marker='*', markersize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: \n",
    "Now let's combine this with our knowledge\n",
    "about downloading streamflow data from USGS!\n",
    "\n",
    "I've provided you with the functions for downloading\n",
    "data that we've used in the past. You don't have to\n",
    "do anything for this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_usgs_url(site_no, begin_date, end_date):\n",
    "    return (\n",
    "        f'https://waterdata.usgs.gov/nwis/dv?'\n",
    "        f'cb_00060=on&format=rdb&referred_module=sw&'\n",
    "        f'site_no={site_no}&'\n",
    "        f'begin_date={begin_date}&'\n",
    "        f'end_date={end_date}'\n",
    "    )\n",
    "\n",
    "def open_usgs_data(site, begin_date, end_date):\n",
    "    url = create_usgs_url((site), begin_date, end_date)\n",
    "    response = urllib.request.urlopen(url)\n",
    "    df = pd.read_table(\n",
    "        response,\n",
    "        comment='#',\n",
    "        skipfooter=1,\n",
    "        delim_whitespace=True,\n",
    "        names=['agency', 'site', 'date', 'streamflow', 'quality_flag'],\n",
    "        index_col=2,\n",
    "        parse_dates=True\n",
    "    ).iloc[2:]\n",
    "\n",
    "    # Now convert the streamflow data to floats and\n",
    "    # the index to datetimes. When processing raw data\n",
    "    # it's common to have to do some extra postprocessing\n",
    "    df['streamflow'] = df['streamflow'].astype(np.float64)\n",
    "    df.index = pd.DatetimeIndex(df.index)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: \n",
    "Now pull out the site id from the `verde_gage`\n",
    "variable. This is contained in the `'STAID'` column, which\n",
    "stands for \"Station ID\". Put this into the variable \n",
    "`station_id`\n",
    "\n",
    "Success on this one should just print out the first 5\n",
    "streamflow values for the Verde river near Campe Verde.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = '2012-10-01'\n",
    "end_date = '2024-09-30'\n",
    "\n",
    "# TODO: Your code here\n",
    "station_id = None\n",
    "\n",
    "site = station_id.values[0]\n",
    "verde_df = open_usgs_data(site, begin_date, end_date)\n",
    "verde_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: \n",
    "Now try pulling out a different gage location\n",
    "using it's name and download the USGS data for the \n",
    "same time period as the `verde_df`. Put this one in\n",
    "`other_gage_df`. Compare the two location's mean\n",
    "streamflows by printing them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: \n",
    "From our original plots of the spatial\n",
    "distribution of gages it was clear that surface\n",
    "water access in Arizona is uneven. For this \n",
    "exercise I want you to count the number of gages\n",
    "in Arizona for each of the HUC8 boundaries. \n",
    "\n",
    "To do this you'll have to iterate over the `huc8`\n",
    "variable using the `huc8.iterrows()` function, which\n",
    "basically loops over each row of the dataframe. \n",
    "Instead of giving you just the row, it also gives \n",
    "you the row column, which is why I have put `i, huc`\n",
    "in the for loop. `i` will keep track of the row number\n",
    "and `huc` will be the actual data from the row.\n",
    "\n",
    "I've got you started on the loop, but your next step\n",
    "is to \"clip\" from `az_gages` the \"geometry\" from the \n",
    "`huc`. Then, count how many gages are in this selection\n",
    "by using the `len` function. Append the result of this\n",
    "to the `number_gages_in_huc` list.\n",
    "\n",
    "Finally, add a new column to the `huc8` dataframe called\n",
    "`'number_gauges'` and set it equal to the `number_gages_in_huc`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_gages_in_huc = []\n",
    "for i, huc in huc8.iterrows():\n",
    "    # TODO: Your code here\n",
    "    clipped_gages = None\n",
    "    n_gages = None\n",
    "    number_gages_in_huc.append(n_gages)\n",
    "\n",
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 11: \n",
    "Finally, plot the number of gages in\n",
    "each HUC - and don't forget to set `add_legend=True`!\n",
    "Use the colormap \"Blues\", and also plot the Arizona\n",
    "outline on top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONGRATULATIONS, you're finished!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "has-tools (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
